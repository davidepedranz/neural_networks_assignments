\section{Theory}
\label{sec:theory}

\subsection{Gradient Descent}
The aim of Stochastic Gradient Descent is to minimize the neural network' cost function with respect to its weights.
The algorithm initialized the weights to random values.
Then, it iteratively select some training example $\nu$ and updates the weights by some fraction $\eta$ (usually called learning rate) of the gradient of the cost function $e^\nu$:
\begin{equation}
    \label{eq:weights-update}
    w_j \leftarrow w_j - \eta \cdot \nabla_{w_j} e^\nu.
\end{equation}

In order to use this iterative procedure, we need to define a proper cost function for the regression problem.
A common choice is to use the squared error for the current example $\varepsilon^\nu$:
\begin{equation}
    \label{eq:cost}
    e^\nu = (\sigma(\varepsilon^\nu) - \tau(\varepsilon^\nu))^2,
\end{equation}
where $\tau(\varepsilon^\nu))$ denotes the correct prediction for the current example.

Another possibility is to use all training examples (or a batch) to perform a single update, i.e. use the following cost function instead of $e$:
\begin{equation}
    E = \frac{1}{P}\frac{1}{2} \sum_{\mu = 1}^{P} (\sigma(\varepsilon^\mu) - \tau(\varepsilon^\mu))^2.
\end{equation}
In this case, we usually talk of Gradient Descent (or Batch Gradient Descent) instead of Stochastic Gradient Descent.

\subsection{Gradient Computation}
The hidden units in out network take as input the training examples $\varepsilon$ and use the hyperbolic tangent as the activation function:
\begin{equation}
    output = tanh(w \cdot \varepsilon)
\end{equation}

Since we have only $2$ hidden units and the output unit simply computes the sum of the outputs of the hidden layer, the final predicted value is computed as:
\begin{equation}
    \sigma (\varepsilon) = tanh(w_1 \cdot \varepsilon) + tanh(w_2 \cdot \varepsilon),
\end{equation}
where $w_1$ and $w_2$ are the weights' vectors.

The gradient of the cost function defined in \cref{eq:cost} with respect to the weights $w_j$ for a single hidden unit $j$ can be computed as follows:
\begin{equation}
    \label{eq:gradient-gen-weight}
    \begin{split}
        \nabla_{w_j} e^\nu =\; & (tanh(w_1 \cdot \varepsilon^\nu) + tanh(w_2 \cdot \varepsilon^\nu) - \tau(\varepsilon^\nu)) \; \cdot \\
        & \cdot (1 - tanh^2(w_j \cdot \varepsilon^\nu)) \cdot \varepsilon^\nu
    \end{split}
\end{equation}
