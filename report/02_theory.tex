\section{Theory}
\label{sec:fundamentals}
As briefly mentioned in the introduction the Perceptron is the first tempt to create a mathematical model of
a neuron and for that reason, exactly like a neuron, it is thought to have incoming and outgoing connections. Since it is a standalone
model the incoming connections are represented by a weights' vector (of total size $N$) and the only outgoing one consists in a value and represents
the output of the Perceptron (i.e. the result of the classification). As touched above this model has $N$ incoming connections,
where $N$ is the dimension of the labels' vector $\xi$ taken as an input (or in the biological context the information coming from the other neurons).\\
The sign of the dot product between these labels ($N$) and the vector containing the weights ($\mathsf{\bm{w}}$) minus a certain threshold
$\theta$ is computed and results in the state (active or inactive) of the perceptron (i.e. the result of the classification problem),
the complete formula is reported below (equation (\ref{perceptron-activation})).

\begin{equation} \label{perceptron-activation}
    S = sign(\mathsf{\bm{w}} \cdotp \xi) = \pm 1
\end{equation}

To train the Perceptron we implemented the Rosemblatt's perceptron algorithm (\cite{rosenblatt1958perceptron}), that consists in
presenting at every step $t$ an input vector $\xi$ to the perceptron, calculate the error as how much the classification was distant from
the correct labeling $\bm{S}$ (through the formula (\ref{perceptron-error})) and update the values of the weights' vector in case the former
calculated error is greater than zero (or in a general case $c$ as we will discuss in the bonus).

\begin{equation} \label{perceptron-error}
    E^\mu = \mathsf{\bm{w}} \cdotp \xi^\mu - S^\mu_R
\end{equation}

The weights update consists in increasing (or decreasing) their previous value by an amount proportional to the error made
by the perceptron while solving the previous classification task. The formula used to computed the values of the updated weight vectors is reported in (equation
\ref{perceptron-weight-update}).

\begin{equation} \label{perceptron-weight-update}
    \mathsf{\bm{w}}(t+1) = \mathsf{\bm{w}}(t) + \frac{1}{N} \big[c - E^{v(t)}\big] \bm{\xi}^{v(t)} S^{v(t)}_R
\end{equation}

The learning process (i.e. the presentation of the input and the update of the weights) is iterated till either the perceptron is able to classify correctly
all the data contained in the training set or it reaches a maximum number of epochs (i.e. iterations).