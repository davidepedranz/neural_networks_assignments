\documentclass[conference]{IEEEtran}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{silence}\WarningsOff[latexfont]

\usepackage{listings}

\usepackage{amsmath}
\usepackage{bm}
\usepackage{dsfont}
\usepackage{amssymb}

\usepackage{graphicx}
\usepackage{cite}
\usepackage{url}
\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage[binary-units,per-mode=symbol]{siunitx}
\sisetup{list-final-separator = {, and }}
\usepackage{booktabs}
\usepackage{pifont}
\usepackage{microtype}
\usepackage{textcomp}
\usepackage[american]{babel}
\usepackage[noabbrev,capitalise]{cleveref}
\usepackage{xspace}
\usepackage{hyphenat}
\usepackage{bm}
\usepackage[draft,inline,nomargin,index]{fixme}
\fxsetup{theme=color}
\usepackage{grffile}
\usepackage{xfrac}
\usepackage{multirow}

\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}
\RequirePackage{xstring}
\RequirePackage{xparse}
\RequirePackage[index=true]{acro}
\NewDocumentCommand\acrodef{mO{#1}mG{}}{\DeclareAcronym{#1}{short={#2}, long={#3}, #4}}
\NewDocumentCommand\acused{m}{\acuse{#1}}

\lstset{language=Matlab,%
%basicstyle=\color{red},
basicstyle=\fontsize{9}{11},
breaklines=true,%
morekeywords={matlab2tikz},
keywordstyle=\color{blue},%
morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},
identifierstyle=\color{black},%
stringstyle=\color{mylilas},
commentstyle=\color{mygreen},%
showstringspaces=false,%without this there will be a symbol in the places where there is a space
numbers=left,%
numberstyle={\tiny \color{black}},% size of the numbers
numbersep=0pt, % this defines how far the numbers are from the text
linewidth=\columnwidth,
emph=[1]{for,end,break},emphstyle=[1]\color{blue}, %some words to emphasise
%emph=[2]{word1,word2}, emphstyle=[2]{style},    
}

\usepackage{blindtext}

\begin{document}

\title{
    Learning by gradient descent\\
    \large Neural Networks and Computational Intelligence - Practical Assignment III
}

\author{
    \IEEEauthorblockN{Samuel Giacomelli}
    \IEEEauthorblockA{\small Student Number: S3546330 \\ s.giacomelli@student.rug.nl}
    \and
    \IEEEauthorblockN{Davide Pedranz}
    \IEEEauthorblockA{\small Student Number: S3543757 \\ d.pedranz@student.rug.nl}
}

\maketitle

\begin{abstract}
    Feed-forward neural networks are powerful devices that can be used to solve regression problems.
    Since a network is formed by many units and the output is a continuous value, the algorithms designed for the perceptron can not be used for the training.
    A possible approach is to define a cost function on the training examples and minimize it using numerical optimization techniques.
    In this assignment, we implement Stochastic Gradient Descent to train a feed-forward network with 1 hidden layer of 2 units.
\end{abstract}

\acresetall

\input{01_introduction}
\input{02_theory}
\input{03_implementation}
\input{04_evaluation}
\input{05_conclusion}
\input{06_work}

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
