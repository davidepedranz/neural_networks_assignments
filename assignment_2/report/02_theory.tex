\section{Theory}
\label{sec:theory}

The aim of the MinOver algorithm is to find the perceptron of maximum stability, i.e. to maximize the margin between the weight vector $\bm{\mathsf{w}}$ and the closest input example $\xi^\mu$. Formally, the stability $\kappa^\mu$ is defined in \cref{eq:perceptron-stability}:

\begin{equation}
    \kappa^\mu = \frac{\mathsf{\bm{w}} \cdotp \xi^\mu S^\mu_R}{\lvert \mathsf{\bm{w}} \rvert}
    \label{eq:perceptron-stability}
\end{equation}

During the training phase, the MinOver algorithm looks for the input vector with the lowest stability at the current time $t$ (\cref{eq:minimum_lookup}) and performs a Hebbian update using its value (\cref{eq:hebbian_update}):

\begin{equation}
    \kappa^{\mu(t)} = \min_\nu \left \{ \kappa^{\nu(t)} =  \frac{\mathsf{\bm{w}}(t) \cdotp \xi^\nu S^\nu_R}{\lvert \mathsf{\bm{w}}(t) \rvert} \right \}
    \label{eq:minimum_lookup}
\end{equation}

\begin{equation}
    \mathsf{\bm{w}}(t+1) = \mathsf{\bm{w}}(t) + \frac{1}{N} \xi^{\mu(t)} S^{\mu(t)}_R
    \label{eq:hebbian_update}
\end{equation}

The procedure is repeated for a certain number of epochs $n_{max}$ or until convergence, i.e. when $\mathsf{\bm{w}}(t+1) = \mathsf{\bm{w}}(t)$.

Like in the Rosenblatt perceptron, the output $S^\mu \in \{+1, -1\}$ for a given examples is computed by evaluating the sign of the dot product between the example $\xi^\mu$ and the weights vector $\mathsf{\bm{w}}$, as shown in \cref{eq:labels_definition}:

\begin{equation}
    S^\mu = sign(\mathsf{w} \cdotp \xi^\mu)
    \label{eq:labels_definition}
\end{equation}
