\section{Evaluation}
\label{sec:evaluation}

\subsection{Storage Capacity}
\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{figures/base}
    \caption{Storage success rate of a Rosenblatt perceptron as a function of $\alpha = P / N$. The experiments use $N = 500$, $n_{max} = 200$ and $n_D = 100$.}
	\label{fig:base}
\end{figure}

\cref{fig:base} shows the results of the base experiment using $N = 500$, $n_{max} = 200$ and $n_D = 100$.
The x-axis represent different values of $\alpha = P / N$, while the y-axis the success rate $Q_{l.s.}$.
As expected, the function looks like a step function from $1$ to $0$.
For $\alpha \approx 1.7$, the success rate $Q_{l.s.}$ drops from $1$ to $0$ very quickly. 

The value of $\alpha$ for which the function drops is called storage capacity of the perceptron.
For $N \to \infty$ (very large number of examples) and $n_{max} \to \infty$ (no limit on the maximum number of training epochs), the theoretical storage capacity of the Rosenblatt perceptron is $\alpha = 2$.

\subsection{Number of Epochs}
\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{figures/multiple_epochs}
    \caption{Storage success rate of a Rosenblatt perceptron as a function of $\alpha = P / N$ for different values of $n_{max}$.}
	\label{fig:multiple_epochs}
\end{figure}

The difference between the theoretical value and the experimental one are mainly due to the limited number of training epochs.
\cref{fig:multiple_epochs} gives an experimental proof of this statement:
for a very small number of epochs (eg. $n_{max} = 10$), the step is close to $\alpha = 1$, while for higher values of epochs the step moves closer and closer to the theoretical value $\alpha = 2$.

\subsection{Number of Dimensions}
\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{figures/multiple_n}
    \caption{Storage success rate of a Rosenblatt perceptron as a function of $\alpha = P / N$ for different values of $N$.}
	\label{fig:multiple_n}
\end{figure}

The theoretical results are valid for $N \to \infty$.
However, real datasets have a limited number of features.
\cref{fig:multiple_n} shows the behaviour of the perceptron for different values of $N$.
For high values of $N$, the shape of the success rate $Q_{l.s.}$ as a function of $\alpha$ is similar to a step function.
For small values of $N$, the function looks like a smoothed step function:
the smaller $N$ is, the higher is the smoothing.

\subsection{Bonus}
\subsubsection{Weight update criterion ($E^\mu < c$)}
\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{figures/bonus_2_c}
    \caption{Storage success rate of a Rosenblatt perceptron as a function of $\alpha = P / N$ for different values of $c$.}
	\label{fig:multiple_c}
\end{figure}
The first experiment we ran to try to understand the contribute of the variable $c$ in the algorithm consisted in training the perceptron for different
values of it. As it is possible to notice in figure \ref{fig:multiple_c} the main effect is a shift to the left of the curve. This result was expected, because
$c$ introduces a step greater than zero in order to proceed to with the update of the weights's vector. What we thought is that if the value needed to perform
the update in greater also the number of epochs needed should be greater, proposing the hypothesis that $c$ works as a simple version of the learning rate that
we find in more complex neural networks.

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{figures/bonus_2_epoch}
    \caption{Storage success rate of a Rosenblatt perceptron as a function of $\alpha = P / N$ for different numbers of $epochs$ with fixed value of $c=2$.}
	\label{fig:fixed_c_multiple_epoch}
\end{figure}
For this reason we decided to simulate the behaviour of the curve for a given value of $c > 0$ and an increasing number of epochs. As it is possible to see
in figure \ref{fig:fixed_c_multiple_epoch} by increasing the number of epochs we \'pushed\' the curve toward the values that we got in the original experiment shown
in figure \ref{fig:base}, contrasting the effect of the increased value of $c$. The results we got from this experiment supported our hypothesis that $c$ behaves like
a learning rate. In fact the algorithm reaches almost the same results for different values of the variables and consistently modified number of epochs as reported in
figure \ref{fig:multiple_c_multiple_epoch}.

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{figures/bonus_2_c_epoch}
    \caption{Storage success rate of a Rosenblatt perceptron as a function of $\alpha = P / N$ for different numbers of $epochs$ and $c$.}
	\label{fig:multiple_c_multiple_epoch}
\end{figure}

\subsection{TODO}
\begin{itemize}
    \item With a limited number of epochs and repetitions the algorithm doesn't have enough time to approach its theoretical values
    \item C seems to be related to the learning rate
    \item say how we choose values for alpha...
    \item Description of plots!
\end{itemize}
