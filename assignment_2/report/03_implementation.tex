\section{Implementation}
\label{sec:implementation}

\subsection{Dataset Generation}
We use an artificially generated dataset with $P$ examples and $N$ features with normally distributed values.
The labels are not chosen randomly, but determined using a teacher weights vector $\bm{\mathsf{w}}^*$ as $S^\mu = sign(\mathsf{w}^{*} \cdotp \xi^\mu)$.
The obtained dataset is by construction linearly separable for $\bm{\mathsf{w}} = \bm{\mathsf{w}}^*$.

We chose $\bm{\mathsf{w}}^*$ such that its norm is constant, for example $||\bm{\mathsf{w}}^*||^2 = N$.
Since we initialize $\bm{\mathsf{w}} = \overrightarrow{0}$, we have that the distance between the student and the teacher perceptron is constant:
\begin{gather*}
    \bm{\mathsf{w}} = \overrightarrow{0} \in \mathbb{R}^N \\
    distance(\bm{\mathsf{w}}, \bm{\mathsf{w}}^*) = || \bm{\mathsf{w}} - \bm{\mathsf{w}}^* || = || \bm{\mathsf{w}}^*|| = \sqrt{N}
\end{gather*}
For simplicity, we chose $\bm{\mathsf{w}} = \overrightarrow{1} \in \mathbb{R}^N$.

\subsection{Perceptron Training}
To implement the MinOver algorithm we started from the code written for the Rosenblatt perceptron in the last assignment.
As for the Rosenblatt perceptron, we initialize the weights vector of the student perceptron with zeros.
Then, we run the training procedure for number of epochs (where $epochs = n_{max} * P$).
At each epoch, we compute the stability of each example in the dataset (for performance reasons, we parallelize the computation as matrix operations instead of iterating over the elements in the dataset) and update the weights using the example with the minimum stability.
We terminate the training if the last update of the weights (normalized by the current norm) is smaller than a certain threshold (we chose $min_{update} = 0.001$), since the update is in practice never equal to zero for numerical problems.

The algorithm differs from the Rosenblatt's one since it performs an update at each step, whether the perceptron is already giving the correct classification or not (it could not have reached the optimum stability).

\subsection{Experiments}
For a fixed value of $P$ and $N$, $n_D$ independent datasets are generated.
A new perceptron is trained on each dataset for at most $epochs = n_{max} * P$ epochs.
For each dataset, we compute the generalization error $\epsilon_g(t_{max})$ as:
\begin{equation}
    \epsilon_g(t_{max}) = \frac{1}{\pi} \arccos \bigg(\frac{\bm{\mathsf{w}}(t_{max})\;\cdotp \bm{\mathsf{w}}^*}{|| \bm{\mathsf{w}}(t_{max}) || \cdot || \bm{\mathsf{w}}^* ||} \bigg)
    \label{eq:generalization_error}
\end{equation}
$\epsilon_g(t_{max})$ measures the distance of the student from the teacher perceptron and gives an indication on the accuracy for the classification of previously unseen data.

Multiple experiments are run for different values of $P$ and $N$ in order to compute $\epsilon_g(t_{max})$ as a function of $\alpha = P / N$.
In other words, we study the accuracy of the perceptron in classifying previously unseen inputs as a function of the rate between the number of examples and the dimension of the input.

\subsection{Noisy dataset}
In real classification problems the data contains some noise.
It is therefore interesting to investigate the performances of the MinOver and Rosenblatt training algorithms on noisy data.
The implementation consists in a small change of the function that generates the dataset (\cref{code:dataset_noise}).
We generate $P$ random values and then define a noise vector which contains a label $-1$ if the the randomly generated number is smaller than $\lambda$ and $+1$ otherwise.
The real labels are then multiplied element-wise with the noise vector.
