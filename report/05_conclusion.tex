\section{Conclusion}

In this assignment we implemented and trained a Rosenblatt perceptron on random generated datasets.
In our experiments, we estimated the probability of success of the perceptron to find a linear separation in a random dataset.
For a limited number of training epochs $n_{max}$ and features $N$, we obtained that the capacity of the Rosenblatt perceptron is about $\alpha = 1.75$ (\cref{subsec:capacity}).

The different from the theoretical results seems to be mainly caused by the limited number of training epochs:
by allowing longer trainings, the plot of the success rate $Q_{l.s.}$ as a function of $alpha$ move to the right and the capacity tends to the theoretical result $\alpha = 2$ (\cref{subsec:epochs}).

The parameter $N$ influences the shape of the function:
for low values of $N$, the function is smooth in the regions before and after the step;
for higher values of $N$, the function gets closer to the theoretical step function (\cref{subsec:dimensions}).

The parameter $c$ in the training algorithm does not influence the performances of the perceptron:
indeed, it only changes the scale of the weights $w$ learned during the training.
However, since each training step in the algorithm can change the weights by at most $\frac{1}{N}\xi^{\mu(t)} S^{\mu(t)}$ (where $\mu(t)$ denotes the current training example), the algorithm will take a longer time to converge for higher values of $c$ (\cref{subsec:c}).

Rosenblatt perceptrons only learns separation hyperplanes that goes through the origin.
In general, the might be hyperplanes that do not go through the origin that linearly separates the dataset.
It is possible to generalize the Rosenblatt perceptron to learn also inhomogeneous separation hyperplanes by adding an artificial dimension to the dataset and forcing it to a non-zero constant (e.g. $1$).
The success rate $Q_{l.s.}$ of inhomogeneous perceptrons seems to be slightly higher than homogeneous ones (\cref{subsec:homogeneous}).
