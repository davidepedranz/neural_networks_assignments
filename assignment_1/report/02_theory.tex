\section{Theory}
\label{sec:fundamentals}

The perceptron is the first mathematical model of a biological neuron.
Like a neuron, it has $N$ incoming connections, which represents the input, and $1$ outgoing connection, which represents the output (i.e. the class of the given example).
The output $S$ is a function of the input $\xi$, defined as the sign of the weighted sum of the input vector:
\begin{equation}
    S = sign(\mathsf{\bm{w}} \cdotp \xi) = \pm 1,
    \label{eq:perceptron-activation}
\end{equation}
where $\mathsf{\bm{w}}$ is an $N$-dimensional vector.
The $sign$ function simulates the activation function of the biological neuron and forces the possible output values to $\pm 1$.

The training process aims at learning the optimal weights vector $\mathsf{\bm{w}}$ to classify correctly all examples.
At time $t$, an example $\xi^t$ is presented to the perceptron:
if the output equals to the correct class $S^t$, $\mathsf{\bm{w}}$ is not changed;
if the output is wrong, a learning step is performed to correct $\mathsf{\bm{w}}$.
The procedure is repeated until the perceptron correctly classifies all the training dataset or until some maximum number of iterations is reached.

The Rosenblatt Perceptron training procedure is formally defined as:
\begin{gather}
    \label{eq:perceptron-weight-update}
    \mathsf{\bm{w}}(t+1) = \mathsf{\bm{w}}(t) + \frac{1}{N} \Theta \big[c - E^{v(t)}\big] \xi^{v(t)} S^{v(t)}, \\
    \Theta[c - E^{\mu}] =  \begin{cases} \label{eq:heaviside-fun}
        1 & E^{\mu} \leq c \\
        0 & E^{\mu} > c
    \end{cases},
\end{gather}
where $E^{v(t)} = \mathsf{\bm{w}}(t) \cdot \xi^{v(t)} S^{v(t)}$.
The weights are initialized as $\mathsf{\bm{w}}(0) = 0$.
The product $\xi^{v(t)} S^{v(t)}$ is called Hebbian term.

The performances of the perceptron are related to the storage capacity of hyperplane defined by the its weights.
For a random dataset with $P$ examples and $N$ dimensions (where $S^\mu = 1$ with probability $1/2$), the theoretical probability of linear separability $P_{l.s.}(P,N)$ is given by:
\begin{equation}
    P_{l.s.}(P,N) =
    \begin{cases}
        \hfil 1 & \text{for} \; P \leq N \\
        2^{1-P} \sum^{N-1}_{i=0} \binom{P - 1}{i} & \text{for} \; P > N       
    \end{cases}
    \label{eq:prob-lin-sep}
\end{equation}

For $N \rightarrow \infty$, $P_{l.s.}(P,N)$ has a well-defined behavior:
\begin{equation} \label{eq:prob-lin-sep-alpha}
    P_{l.s.}(P,N) =
    \begin{cases}
        1 & \text{for} \; \alpha \leq 2 \\
        0 & \text{for} \; \alpha > 2
    \end{cases},
\end{equation}
where $\alpha = P / N$.
