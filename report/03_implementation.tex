\section{Implementation}

\subsection{Dataset Generation}
The first step of the experiment is to generate an artificial dataset with $P$ random N-dimensional feature vectors and binary labels $\mathcal{D} = \{ \xi^\mu, S^\mu \,|\, \xi^\mu \in \mathds{R}^N, S^\mu \in \{+1, -1\} \}_{\mu=1}^P$.
The feature vectors $\xi^\mu$ have independent random components $\xi_j^\mu \sim \mathcal{N}(0, 1)$ and are generated using the \texttt{randn} command in Matlab.
The labels $S^\mu$ are assume the values $\{+1, -1\}$ with equal probability of $1/2$.

\begin{lstlisting}[language=Matlab]
  function [X, y] = generate_dataset(P, N)
      X = randn(P, N);
      y = iff(rand(P, 1) < 0.5, -1, 1);
  end
\end{lstlisting}

\subsection{Perceptron Training}
The next step is to use the Rosenblatt algorithm to implement and train a perceptron on the generated dataset.
The Rosenblatt algorithm is a sequential procedure that learns the weights of the perceptron from the examples.
The weights are initialized to zero, then the training examples are presented one by one to the perceptron for a given number of epochs (iterations).
If the example is correctly classified according to the current weights, no update is performed;
if the example if wrongly classified, the weights are updated according to the following rule:
\begin{gather*} 
    w(0) = 0, \\
    w(t + 1) =  \begin{cases}
                    w(t) + \frac{1}{N} \xi^{\mu(t)} S^{\mu(t)} &\text{if $E^{\mu(t)} \leq 0$}\\
                    w(t) &\text{else}
                \end{cases},
\end{gather*}
where $E^{\mu(t)} = w(t) \xi^{\mu(t)} S^{\mu(t)}$, $t = 1, 2, ...$ represents the current time step and $\mu(t) = 1, 2, ..., P, 1, 2, ...$ identifies the current example.
The algorithm is stopped either when $E^\mu > 0$ for all examples in the dataset or the maximum number of epochs $n_{max}$ is reached.

\subsection{Experiments}
For a fixed value of $P$ and $N$, $n_D$ independent datasets are generated.
A new perceptron is trained on each dataset for at most $n_{max}$ epochs.
For each dataset, we compute the rate fraction $Q_{l.s.}$ of successful runs, where a run is successful if the perceptron correctly classifies each example in the dataset at the end of the training phase.
$Q_{l.s.}$ gives an estimate of the probability that the perceptron finds a linear separation in a random dataset of $P$ independent points in $N$ dimension.

Multiple experiments are run for different values of $P$ and $N$ in order to compute $Q_{l.s.}$ as a function of $\alpha = P / N$.
In other words, we study the probability of the perceptron to find a linear separation as a function of the rate between the number of examples and the dimension of the input.

\subsection{Bonus parts}
\subsubsection{Weight update criterion ($E^\mu < c$)}
For this point we modified the function that trains the perceptron in a way which assures that the update of the weights' vector $\mathsf{\bm{w}}$ is 
performed just if the error computed using the formula \ref{perceptron-error} is smaller than a newly introduced value $c$. Furthermore we had to update
the line of code that checks wether all the classifications performed by the perceptron were right by imposing the condition that they had to be greater than c.

\subsubsection{Inhomogeneous perceptron solution}
The inhomogeneous solution of the perceptron consists in the solution of the classification problem using a perceptron that is built with the idea that the line that determines
the separation between the two classes doesn't pass through the origin, but has a different intercept. In order to express this property of the line and element, with value $-1$ has to be added to
end of the input vector $\xi$ and also the weights' vector size has to be increased by one; the last element will be treated as the intercept $\theta$. We modified the function that runs
our experiment so that the training function already receives in input two vectors $N$ and $\mathsf{\bf{w}}$ with the correct sizes.