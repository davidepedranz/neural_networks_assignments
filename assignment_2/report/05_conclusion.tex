\section{Conclusion}
\label{sec:conclusion}

The MinOver algorithm tries to maximize the minimum stability of the dataset.
In absence of noise, MinOver yields to a generalization error similar to the Rosenblatt algorithm.
For both algorithms the generalization error decreases with a higher number of examples $P$ and approaches zero in the limit for $P \to \infty$.
In presence of noise, the MinOver algorithm diverges for big datasets, where the likelihood of the data to be non linearly separable is very high.

In the original formulation, MinOver does not give any advantages over the Rosenblatt algorithm in terms of generalization error, both in absence and presence of noise in the data.
In addition it turned out that the MinOver algorithm requires more computation time than the Rosenblatt one, because for each example an iteration over all the data contained in the
dataset is needed in order to compute the value of the stability.