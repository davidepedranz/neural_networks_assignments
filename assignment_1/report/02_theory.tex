\section{Theory}
\label{sec:fundamentals}

The perceptron is the first mathematical model of a biological neuron.
Like a neuron, it has $N$ incoming connections, which represents the input, and $1$ outgoing connection, which represents the output (i.e. the class of the given example).
The output $S$ is a function of the input $\xi$, defined as the sign of the weighted sum of the input vector:
\begin{equation}
    S = sign(\mathsf{\bm{w}} \cdotp \xi) = \pm 1,
    \label{eq:perceptron-activation}
\end{equation}
where $\mathsf{\bm{w}}$ is an $N$-dimensional vector.
The $sign$ function simulates the activation function of the biological neuron and forces the possible output values to $\pm 1$.

The training process aims at learning the optimal weights vector $\mathsf{\bm{w}}$ to classify correctly all examples.
At time $t$, an example $\xi^t$ is presented to the perceptron:
if the output equals to the correct class $S^t$, $\mathsf{\bm{w}}$ is not changed;
if the output is wrong, a learning step is performed to correct $\mathsf{\bm{w}}$.
The procedure is repeated until the perceptron correctly classifies all the training dataset or until some maximum number of iterations is reached.

The Rosenblatt Perceptron training procedure is formally defined as:
\begin{gather}
    \label{eq:perceptron-weight-update}
    \mathsf{\bm{w}}(t+1) = \mathsf{\bm{w}}(t) + \frac{1}{N} \Theta \big[c - E^{v(t)}\big] \xi^{v(t)} S^{v(t)}, \\
    \Theta[c - E^{\mu}] =  \begin{cases} \label{eq:heaviside-fun}
        1 & E^{\mu} \leq c \\
        0 & E^{\mu} > c
    \end{cases},
\end{gather}
where $E^{v(t)} = \mathsf{\bm{w}}(t) \cdot \xi^{v(t)} S^{v(t)}$.
The weights are initialized as $\mathsf{\bm{w}}(0) = 0$.
The product $\xi^{v(t)} S^{v(t)}$ is called Hebbian term.

In this work, we want to investigate the storage capacity of the perceptron as a function of the number of examples $P$ and the number of dimensions $N$.
In \cref{eq:prob-lin-sep} we report the theoretical limit expressed in form of $P_{l.s.}(P,N)$, the probability for linear separability (random $S^\mu = \pm 1$, with probability $0.5$).

\begin{equation} \label{eq:prob-lin-sep}
    P_{l.s.}(P,N) = \begin{cases}
        \hfil 1 & \text{for} \; P \leq N\\
        2^{1-P} \sum^{N-1}_{i=0} \binom{P - 1}{i} & \text{for} \; P > N       
    \end{cases}
\end{equation}

For $N \rightarrow \infty$, $P_{l.s.}(P,N)$ has a well-defined behavior, which can be summarized by \cref{eq:prob-lin-sep-alpha} in terms of $\alpha = P / N$.

\begin{equation} \label{eq:prob-lin-sep-alpha}
    P_{l.s.}(P,N) = \begin{cases}
        1 & \text{for} \; \alpha \leq 2\\
        0 & \text{for} \; \alpha > 2
    \end{cases}
\end{equation} 