\section{Theory}
\label{sec:fundamentals}
The Perceptron is the first attempt to create a mathematical model of a biological neuron:
like a neuron, it has incoming and outgoing connections.
Since it is a standalone model the incoming connections are represented by a weights' vector and the only outgoing one consists in a value and represents the output of the Perceptron (i.e. the result of the classification).
The perceptron has $N$ incoming connections, where $N$ is the dimension of the labels' vector $\xi$ taken as an input (or in the biological context the information coming from the other neurons).

The sign of the dot product between these labels ($N$) and the vector containing the weights ($\mathsf{\bm{w}}$) minus a certain threshold $\theta$ is computed and results in the state (active or inactive) of the perceptron (i.e. the result of the classification problem), the complete formula is reported below (\cref{eq:perceptron-activation}).

\begin{equation}
    S = sign(\mathsf{\bm{w}} \cdotp \xi) = \pm 1
    \label{eq:perceptron-activation}
\end{equation}

The training process consists in presenting at every step $t$ an input vector $\xi$ to the perceptron, compute the error as how much the classification was distant from the correct labeling $\bm{S}$ (\cref{eq:perceptron-error}) and update the values of the weights' vector in case the former calculated error is greater than zero (or in general $c$, with $c > 0$).

\begin{equation}
    \label{eq:perceptron-error}
    E^\mu = \mathsf{\bm{w}} \cdotp \xi^\mu - S^\mu_R
\end{equation}

The weights update consists in increasing (or decreasing) their previous value by an amount proportional to the error made by the perceptron while solving the previous classification task.
The formula used to computed the values of the updated weight vectors is reported in \cref{eq:perceptron-weight-update}.

\begin{equation}
    \label{eq:perceptron-weight-update}
    \mathsf{\bm{w}}(t+1) = \mathsf{\bm{w}}(t) + \frac{1}{N} \big[c - E^{v(t)}\big] \bm{\xi}^{v(t)} S^{v(t)}_R
\end{equation}

The learning process (i.e. the presentation of the input and the update of the weights) is iterated until either the perceptron is able to classify correctly all the data contained in the training set or it reaches a maximum number of epochs (i.e. iterations).
