\section{Theory}
\label{sec:theory}

This time the focus was to find the perceptron of \textbf{maximum stability}, which means trying to maximize the margin between the
weight vector and the closest input label, or in other words trying to approximate to the teacher perceptron $\bm{\mathsf{w}}^*$.
The stability is therefore defined exactly as the distance mentioned former using the formula [\ref{eq:perceptron-stability}].

The dataset used is slightly different from the one of the Rosenblatt algorithm and the difference consists in defining the labels
as the sign of the dot product between the input and the vector that represents the teacher perceptron (i.e. the result of the
classification performed by the teacher perceptron for each input) (Formula [\ref{eq:labels_definition}]). 

The stability is calculated for every input vector contained in the dataset and, after finding all the values of $\kappa$,
the minimum is searched (Formula [\ref{eq:minimum_lookup}]) and a Hebbian update (Formula [\ref{eq:hebbian_update}]) is performed using the input vector
respect to that the perceptron has the minimum stability, the in order to maximize the value of that property.

This procedure is iterated over the number of epochs $n_{max}$ and stopped before reaching that value just if the last update
corresponds to a zero update (i.e. $\bm{\mathsf{w}}(t+1) = \bm{\mathsf{w}}(t)$). 

\begin{equation} \label{eq:labels_definition}
    S^\mu = sign(\bm{\mathsf{w}} \cdotp \xi^\mu)
\end{equation}

\begin{equation} \label{eq:perceptron-stability}
    \kappa^\mu = \frac{\bm{\mathsf{w}} \cdotp \xi^\mu S^\mu_R}{\lvert \bm{\mathsf{w}} \rvert}
\end{equation}

\begin{equation} \label{eq:minimum_lookup}
    \kappa^{\mu(t)} = \min_\nu \left \{ \kappa^{\nu(t)} =  \frac{\bm{\mathsf{w}} \cdotp \xi^\mu S^\mu_R}{\lvert \bm{\mathsf{w}} \rvert} \right \}
\end{equation}

\begin{equation} \label{eq:hebbian_update}
    \bm{\mathsf{w}}(t+1) = \bm{\mathsf{w}}(t) + \frac{1}{N} \xi^{\mu(t)} S^{\mu(t)}_R
\end{equation}