\section{Conclusion}
\label{sec:conclusion}

The MinOver algorithm tries to maximize the minimum stability of the dataset.
In absence of noise, MinOver yields to a generalization error similar to the Rosenblatt algorithm.
For both algorithms the generalization error decreases for a high $\alpha = P / N$ (i.e. with a higher number of examples $P$) and approaches zero in the limit for $\alpha \to \infty$.
In presence of noise, the MinOver algorithm diverges for big datasets, where the likelihood of the data to be non linearly separable is very high.

In the original formulation, MinOver does not give any advantages over the Rosenblatt algorithm in terms of generalization error, both in absence and presence of noise in the data.
In addition, the MinOver algorithm requires more computation power than the Rosenblatt one, since it needs to compute the stability of all examples in the dataset at each epoch.
